\chapter{LITERATURE REVIEW}\label{chapter2}
Essentially, the results in the range of alternatives to reconstruct cities vary according to the definition of three main phases: (i) sensors and an appropriate measurement of the targets, (ii) processing and classification, according to a desired level of detail and (iii) standardization in well-established formats, such as CityGML \cite{kolbe2005}. The following sections introduce the main works and methodologies in 3D reconstruction of buildings and facades, as well as the spatial and spectral characteristics usually found in these environments, which represent the great challenges of the area.

\section{Structural data}\label{aquisicao} 
Structural data is considered here as all data that carries with it geometric information of the scene, that after being assigned to a valid coordinate system, can present geometric values very close to the reality. In the following section, it is presented a summary of current platforms and sensors for acquiring this information.

\subsection{Common alternatives for structural data acquirement}
Urban environments are easily recognized in images by its linear aspects, repetitive patterns, with structures frequently devoid of natural elements, with a wide variety of sizes, shapes, compositions and arrangements. Applications involving studies of these environments equally demands a high resolution acquisition, usually performed on a large or small scale. In Table \ref{tabela-dado-estrutural}, is shown a schema regarding conventional types of structural data acquisition and the maximum Level of details (LoD) that each configuration could reach in terms of urban objects. The list makes a balance between their respective costs, knowledge required to manage the equipment, accuracy, spatial and spectral resolutions, among others. It is important to note the respective table is a non-exhaustive list, which highlights only common forms of acquirement. Other forms of acquisition, such as civil construction cranes, onboard balloons, helicopters, and any other resources are not listed. 
\begin{table}[H]
    \renewcommand{\arraystretch}{1.4}
    \caption{Maximum LoD and quality of 3D urban models according to the acquisition and sensor characteristics.}
    \tiny \centering
    \begin{tabular}{C{0.01cm}C{0.02cm}C{0.6cm}C{0.6cm}L{0.6cm}C{1.3cm}C{0.5cm}C{0.5cm}C{0.5cm}C{0.6cm}C{0.6cm}C{0.5cm}C{0.7cm}C{0.7cm}L{0.7cm}}
        \toprule      
        
        \multicolumn{3}{c}{\multirow{2}{*}{Plataform}}  & \multirow{2}{*}{\parbox{0.7cm}{\centering Spectral region}} & \multirow{2}{*}{\parbox{0.6cm}{\centering Spatial resol.}} & \multirow{2}{*}{\parbox{1.3cm}{\centering Sensor view}} & \multicolumn{3}{c}{Building parts} & \multirow{2}{*}{\parbox{0.6cm}{\centering Max. LoD}} & \multirow{2}{*}{\parbox{0.6cm}{\centering Costs}} & \multirow{2}{*}{Operated} & \multirow{2}{*}{PC} & \multirow{2}{*}{Software} & \multirow{2}{*}{\parbox{0.7cm}{\centering Accuracy}}  \\ \cmidrule{7-9}
        & & & & & & Roof & Facade & Indoor &&&&&& \\ 
        
        \toprule      
        
        \multicolumn{2}{c}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{Orbital}}}} & Satellite& Optical &  \textbullet\textbullet\textbullet\color{lightgray}{\textbullet}  & Multi-view & \cellcolor{green!20}\checkmark & $\times$ & $\times$ & \cellcolor{blue!15}LoD2 & \faDollar\faDollar\faDollar & \faGraduationCap & $\star\star\star$ & - &  \textbullet\textbullet\textbullet\color{lightgray}{\textbullet} \\
        
        & & Satellite& Laser & \textbullet\textbullet\textbullet\color{lightgray}{\textbullet} & Multi-view & \cellcolor{green!20}\checkmark & $\times$ & $\times$ & \cellcolor{blue!15}LoD2 & \faDollar\faDollar\faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\textbullet\color{lightgray}{\textbullet}\\ 
        
        & & Satellite& Hybrid & \textbullet\textbullet\textbullet\color{lightgray}{\textbullet} & Multi-view  & \cellcolor{green!20}\checkmark & $\times$ & $\times$ & \cellcolor{blue!15}LoD2 & \faDollar\faDollar\faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\textbullet\color{lightgray}{\textbullet} \\ \hline
        
        \multirow{9}{*}{\rotatebox[origin=c]{90}{\textbf{Aerial (multiple ranges)}}} & \multirow{4}{*}{\rotatebox[origin=c]{90}{long}} & Airplane & Optical & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Nadir & \cellcolor{green!20}\checkmark & \cellcolor{yellow!20}\checkmark$~\star$ & $\times$ & \cellcolor{yellow!15}LoD2$\star$ & \faDollar\faDollar\faDollar & \faGraduationCap & $\star\star\star$  & - & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet}\\
        
        & & Airplane & Laser & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Nadir & \cellcolor{green!20}\checkmark & \cellcolor{yellow!20}\checkmark$~\star$ & $\times$ & \cellcolor{yellow!15}LoD2$\star$ & \faDollar\faDollar\faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} \\
        
        & & Airplane & Hybrid & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar\faDollar\faDollar & \faGraduationCap & \checkmark & - & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} \\
        
        & & Airplane & Optical & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar\faDollar & \faGraduationCap & $\star\star\star$  & - & \textbullet\textbullet\textbullet\color{lightgray}{\textbullet} \\ \hhline{~~-------------}
        
        & \multirow{2}{*}{\spheading[5em]{\vfill medium}} & UAV & Optical & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar & \faGraduationCap & $\star\star\star$  & - & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} \\
        
        & & UAV & Laser & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Nadir & \cellcolor{green!20}\checkmark & $\times$ & $\times$ & \cellcolor{blue!15}LoD2 & \faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} \\ 
        
        & & UAV & Hybrid & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\color{lightgray}{\textbullet}\color{lightgray}{\textbullet} \\ \hhline{~~-------------}
        
        & \multirow{2}{*}{\parbox{0.1cm}{\rotatebox[origin=c]{90}{close}}} & UAV & Optical & \textbullet\textbullet\textbullet\textbullet & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & - & \faUser & $\star\star\star$  & - & \textbullet\textbullet\textbullet\textbullet\\
        
        & & UAV & Laser & \textbullet\textbullet\textbullet\textbullet & Multi-view & \cellcolor{green!20}\checkmark & \cellcolor{green!20}\checkmark & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\textbullet\textbullet \\ \hhline{---------------}
        
        \multicolumn{2}{c}{\multirow{5}{*}{\rotatebox[origin=c]{90}{{\textbf{Terrestrial}}}}} & User & Optical & \textbullet\textbullet\textbullet\textbullet & Multi-view & $\times$ & \cellcolor{yellow!20}\checkmark$~\star\star$ & $\times$ & \cellcolor{blue!30}LoD3 & - & \faUser & $\star\star\star$  & - & \textbullet\textbullet\textbullet\textbullet \\
        
        & & User & Laser & \textbullet\textbullet\textbullet\textbullet  &  Multi-view & $\times$ & \cellcolor{yellow!20}\checkmark$~\star\star$ & $\times$ & \cellcolor{blue!30}LoD3 & \faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\textbullet\textbullet \\
        
        & & User & Optical & \textbullet\textbullet\textbullet\textbullet & Multi-view & $\times$ & $\times$ & \cellcolor{green!20}\checkmark & \cellcolor{blue!45}LoD4 & - & \faUser & $\star\star\star$  & - & \textbullet\textbullet\textbullet\textbullet \\
        
        & & User & Laser & \textbullet\textbullet\textbullet\textbullet & Multi-view & $\times$ & $\times$ & \cellcolor{green!20}\checkmark & \cellcolor{blue!45}LoD4 & \faDollar & \faGraduationCap & \checkmark & \faDownload & \textbullet\textbullet\textbullet\textbullet \\      
        
        \bottomrule     
    \end{tabular}
    \label{tabela-dado-estrutural} 
    \vspace{0.1cm}
    \begin{flushleft}
        \scriptsize{\textbf{Low}, \textbf{Medium}, \textbf{High}, and \textbf{Very high}, respectively \textbf{(\textbullet\textbullet\textbullet\textbullet)};}\\  \vspace{0.1cm}
        \scriptsize{\textbf{Estimated cost (\faDollar) - 0} to \textbf{3}; \textbf{Specialist (\faGraduationCap)}; \textbf{Common user (\faUser)}; \textbf{Embedded (\faDownload)}.}\\ \vspace{0.15cm}
        \scriptsize{$\star$ Parameter is a function of the sensor's Field of View (FOV). In cases of comprehensive FOVs, it is possible to observe not only the characteristics of roofs, but also of facades, allowing an acquisition of values above LoD2. In cases of FOVs with narrow angles, only the building footprint are observable, consequently, only LoD1 is reachable.}\\
        \scriptsize{$\star\star$ The quality of facades images by terrestrial platforms, depends directly on the height of the building. The imaging of buildings with a height greater than 50 meters, for instance, is affected by the acquisition geometry in its upper part, as well as inner structures.}\\
        \scriptsize{$\star\star\star$ Control points are required.}\\      
    \end{flushleft}
    \FONTE{Author's production.}
\end{table}

The imaging of urban environments, especially the analysis of facades, are carried out conventionally by orbital, aerial (long, medium and close-range) and terrestrial platforms. Orbital platforms usually have a common goal, the imaging of areas whose spectral and spatial variability are low, e.g. forests and oceans. Although in orbit, some sensors has centimeter scales, which places it as one of the alternatives in the urban monitoring. The data are often on private or military domain, when not, with high costs or requiring some kind of pre-processing due to atmospheric attenuation. Observing the table, it can be seen that the three different orbital configurations have characteristics whose facade observation is possible, but only fine resolutions would allow them to observe the details, which is not always possible by this type of alternative. When available, both images and laser measurements are rare and inaccessible to the urban context.

Aerial platforms, are categorized here into three segments: long, medium and close-range. Surveys whose sensor is onboard airplanes, flying over altitudes around 6,000 meters, are usually considered as long-range surveys. They are, in fact, the most used category over the years, allowing sophisticate imaging, sometimes, with hybrid sensors and configurations that adequately serve the innumerable urban activities. Although many of these products still present high costs, aerial images are still the most widely used in large-scale studies, since it does not require a detailed analysis.

Different imaging requires different costs, efforts and technical infrastructure. The estimated cost shown in Table \ref{tabela-dado-estrutural} is relative and may vary according to area coverage. For example, the cost of orbital laser imaging or high-resolution optical sensors requires a high cost, but may benefit from covering an area whose terrestrial imaging would cover only in parts. The estimated cost in table, therefore, is the absolute cost.

On medium-range, the surveys are normally carried out by fixed-wing Unmanned Aerial Vehicles (UAV) or even by airplanes, whose altitude does not exceed 1,000 meters. In this category, remote data naturally has a gain in resolution, in addition to enabling large-scale imaging, e.g. farms, forests, neighborhoods and others. In facade analysis, only sensors with wide FOV or multi-views gives guarantees of imaging these areas. In Brazil, however, this category still goes through standardization and other bureaucratic procedures for practice, although it is regularly offered by numerous companies. 

Finally, close-range surveys consists of small and medium-sized platforms, with target's distance around 200 meters. This specific configuration has gained interest in areas such as Agriculture and Cartography. Close-range devices, such as the UAV (commonly called drones), are usually used for its flexibility, low cost and stability in flight over narrow paths. These qualities, makes this alternative a right alternative to image all the faces of a particular building (e.g. roof, facades, inner gardens). Contrary to the lasers sensors, the UAVs is not only accessible to experts, but also to common users. These small devices are easily purchased on the market for recreational use, but depending on their physical characteristics, they can also be adapted for scientific studies. The handling of laser scanners, however, requires a certain expertise, not just onboard UAVs, but on any other platform. 

The terrestrial platforms complete the range of alternatives in close-range imaging, where three main platforms are adopted: total-station, vehicles or even carried by the own specialist. Although the ground survey does not require the use of an aerial platform, this type of imaging has almost the same properties as the close-range aerial survey. In this category, the images are taken with lateral geometry, at the lower sight level, interesting configuration for facade analysis, but not feasible when the purpose requires the complete coverage of the building. For example, indoor gardens, roof, back yards among other structures are hardly observed.

The green color in table, shows the features benefited by the respective configuration, while the yellow determines the dependence of some technical factor. In blue, the maximum LoD reachable by the respective configuration, where the lighters denotes non-detailed 3D models, and darkers, detailed ones. In indoor surveys, measures are taken as a complement to lower levels, for instance, LoD2 and LoD3. Of course, the LoD4 model shown would be obtained in addition to an existing model. Measures indoor by itself, do not support the generation of LoD1, LoD2 or LoD3.

\subsection{Other alternatives for structural data}   
The generation of 3D urban models are mainly archived using remote systems, either by active or passive sensors, embedded on a long or close-range acquisition platforms, which is determined often by the application purposes, as presented in the previous section. In addition, complete cities may also be reconstructed from instruments other than those previously presented, for example, by radar (Radio Detection and Ranging), which by its imaging properties, normally allow studies such as terrain variations using SAR (Synthetic Aperture Radar) Interferometry, topography analysis under forest regions, and also cities reconstruction by the technique known as Persistent Scattered Interferometry (PSI) \cite{ferretti2001}, that despite presenting accurate results, it is still expensive technology for requiring systematic radar surveys \cite{schunert2016, schack2012}.

Beyond Photogrammetry, laser, and radar reconstruction techniques, cities can also be generated by procedural models or even manually\footnote{Reconstruction is the process of creating digital representations as close as possible to the measurement in terms of accuracy \cite{musialski2013}. While generation consists of artificially creating realistic representations given a set of rules or procedural mechanisms \cite{muller2006}.}. The concept of virtual and smart cities has become even more common over the years, and has stimulated the continuous development of standalone applications independent of the imaging devices or platform.

\section{Urban environments and their many representations}\label{urban-environment}
The different categories of artificial coverage (man-made) constantly change in small fractions of space and time, often altered by humans as well. Understanding the aspects of texture, geometry, material, architectural styles, coverage, among other physical properties helps define the level of abstraction in the method to be developed \cite{vangool2013}. The following sections briefly discuss some of the geometric aspects of buildings and their features, in order to contextualize the main factors for 3D modeling and reconstruction of these environments. For a more comprehensive reading, it is recommended the work of \cite{musialski2013}.

\subsection{Buildings} 
Regardless the type of imaging, artificial structures are easily distinguished from natural mostly by their linear characteristics. Areas of vegetation are generally identified by their heterogeneous texture and typical spectral properties, for example, the numerous vegetation indices, which often allow to separate adequately between vegetation and artificial coverage \cite{gerke2014, demir2012}. However, in some cases, vegetation mixes with urban elements, such as terrace gardens or vertical gardens on balconies, aspects that make it difficult to classify them at the spectral level, but could be easily categorized as being part of a building with the volumetry complement.

A 3D urban model is a representation of an urban environment with 3D geometries of common urban objects and structures, with buildings as the most prominent feature \cite{billen2014, lancelle2010}. The perception of a building through the human visual system provides innumerable premises on which characteristics should be considered at first. Although the methodology presented in this study does not use spectral or geometrical properties of buildings to extract their features, some of their characteristics are discussed in this section.

A building consists of a cubic element formed by a flat roof or one or more faces \cite{tutzauer2015, haala1999}. Some of them have dome or arched characteristics, and may overlap each other (American type) or present spectral variations due to their composition. In Figure \ref{building-roofs-topologies}, typical roofing examples are illustrated, where it is possible to find from the simplest form (Figure \ref{telhado-a}), to a more complex ones (Figure \ref{telhado-e} and \ref{telhado-f}). These characteristics are essential when the extraction of information is carried out essentially by aerial images. 
\begin{figure}[H]
    \centering  
    \caption{Typical representations of roofs and building topologies. (a) Plane roof. (b) One-face roof. (c) Two-faces roof. (d) Four-faces roof. (e) Arched and (f) domed. (g) and (h) Parametric shape. (i) Prismatic. (j) Polyhedral. (l) Curved and (k) free-form.}
    \vspace{6mm}
    \subfigure[]{\label{telhado-a}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-a.pdf}}    
    \subfigure[]{\label{telhado-b}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-b.pdf}} 
    \subfigure[]{\label{telhado-c}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-c.pdf}} 
    \subfigure[]{\label{telhado-d}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-d.pdf}}
    \subfigure[]{\label{telhado-e}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-e.pdf}}
    \subfigure[]{\label{telhado-f}\includegraphics[width=0.15\textwidth]{\figspath/telhados/telhados-f.pdf}} \\
    \subfigure[]{\label{classes-building-a}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-a.pdf}}\hspace{0.1cm}
    \subfigure[]{\label{classes-building-b}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-b.pdf}}\hspace{0.1cm}
    \subfigure[]{\label{classes-building-c}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-c.pdf}}\hspace{0.1cm}
    \subfigure[]{\label{classes-building-d}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-d.pdf}}\hspace{0.1cm}
    \subfigure[]{\label{classes-building-e}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-e.pdf}}\hspace{0.1cm}
    \subfigure[]{\label{classes-building-f}\includegraphics[width=0.15\textwidth]{\figspath/classes-building/classes-building-f.pdf}}
    \vspace{2mm}
    \legenda{}    
    \label{building-roofs-topologies}   
    \FONTE{Adapted from \citeonline{haala1999} and \citeonline{brenner2000}.}    
\end{figure}

\citeonline{brenner2000} considers six different models of building classes: parametric, combined, prismatic, polyhedral, curved and free-form (Figures \ref{classes-building-a} to \ref{classes-building-f}). Parametric buildings are the simplest forms, usually referring to houses or large sheds, sometimes combining one another (Figure \ref{classes-building-b}), forming large complexes, usually present in industrial regions, in which buildings tend to have a sparse disposition. The prismatic model, frequent in commercial areas, represent buildings whose roof is flat and not necessarily in rectangular forms. Polyhedral models expose the geometric details, such as chimneys, eaves, different level of roofs, among others. The curved model, is commonly observed in churches or religious buildings. The other buildings, whose shape does not have a defined pattern, are characterized as free forms, such as the Gherkin building in London, England, or the famous Copan building, in São Paulo, Brazil.

In real world, buildings are complex structures with different orientations, slopes, shapes, textures and compositions. In addition, tasks involving the extraction of facades features can be impaired by the presence of other objects surrounding, such as cars, lighting poles, pedestrians, and trees \cite{verdie2015, cheng2011}. They are located too close to the walls, which could be treat either as an occluder object or could be simply modeled as being part of the wall.

Besides to the physical properties of buildings, the choice of platform and sensor for imaging define, for example, which areas of the building are better imaged than the others. The Figure \ref{building-field-of-view-1} illustrated a close-range acquisition through multiple views. The faces in gray represent the regions not observed by the respective type of acquisition, while red consists the ones that totally imaged. Then, the best configuration for buildings, according to the picture, is the multi-view and terrestrial close-range acquisition (Figure \ref{building-field-of-view-1d}).
\begin{figure}[ht]
    \centering     
    \caption{Building's faces benefited according to the platform. (a) Sensors with geometry to nadir, allow to observe only roofs (some sensors, however, have wide FOVs that allow the analysis of adjacent facades). (b) Sensors with multi-view. A trend in urban mapping by aerial platforms. Oblique images provide wide imaging coverage. (c) Ground sensors on board mobile platforms. They allow the observation of details in high resolution. (d) Urban hybrid imaging by terrestrial sensors and close-range UAVs. They allow the observation of all the faces of buildings, however, still with high operational cost.}	 
    \vspace{6mm}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{\figspath/building-field-of-view/building-field-of-view-01.png}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{\figspath/building-field-of-view/building-field-of-view-02.png}}
    \subfigure[]{\includegraphics[width=0.24\textwidth]{\figspath/building-field-of-view/building-field-of-view-03.png}}
    \subfigure[]{\label{building-field-of-view-1d}\includegraphics[width=0.24\textwidth]{\figspath/building-field-of-view/building-field-of-view-04.png}}
    \includegraphics[width=1\textwidth]{\figspath/building-field-of-view/building-field-of-view.pdf}
    \vspace{2mm}
    \legenda{}    
    \label{building-field-of-view-1}    
    \FONTE{Author's production.}
\end{figure}

\subsection{Facade features}\label{facade-features}
The reconstruction of facades fulfills an important segment in inspecting and enforcing urban planning laws. For instance, the mapping of facade features\footnote{Sometimes called openings and referenced here as facade features, such as doors, windows, balconies, gates, etc.} could assist whether a new building can be erected in front or at a given distance from a reference point. \cite{burochin2014} analyzed the facade characteristics in order to validate building constructions in accordance to French planning laws. Not only is the geometry of buildings important, but their semantics as well. For the validation of urban plans it is essential that windows and doors are not only geometrically represented but also explicitly labeled as such \cite{tutzauer2015}.

In this study, however, only the details on the building facades are interested, and so far, there is no better way than close-range imaging to observe those details (as exemplified in Figure \ref{building-field-of-view-1}). The campaigns of urban data via terrestrial platforms benefits from the rich information collection, but it is a disadvantage as it does not allow a wide observation of the structure, such as their internal architecture, roof and, depending to their height, the upper part. To illustrate this issue, the diagram in the Figure \ref{building-field-of-view-2} shows each of these non-imaged features. The points $A$ and $D$, the high structures that, due to the limited FOV, might only be partially observed. The point $B$, the roof characteristics, which are hardly observed by terrestrial configurations. Finally, the point $C$, another example of structures that could not be observed by such imaging.
\begin{figure}[H]
    \centering     
    \caption{Diagram representing a typical terrestrial campaign. In particular, points A and D denote areas that were negatively affected. High buildings are only partially observed in this type of acquisition. B and C, show details of characteristics that can not be observed either by the type of imaging (only street-side view), or because they are internal structures, such as winter gardens - highlighted in C.}
    \vspace{6mm}
    \includegraphics[width=1\textwidth]{\figspath/building-field-of-view/building-field-of-view-5.pdf}
    \includegraphics[width=1\textwidth]{\figspath/building-field-of-view/building-field-of-view.pdf}
    \vspace{2mm}
    \legenda{} 
    \label{building-field-of-view-2}   
    \FONTE{Author's production.}
\end{figure}

The characteristics of doors and windows are pretty much distinguishable, a rectangular geometric pattern, sometimes, occluded by cars, poles, or other objects. The structure's texture uniformity and repeatability of such openings could be verified by the use of radiometric statistics, Histogram of Oriented Gradient (HOG), and Gradient Accumulation Profile (GAP), as in \citeonline{burochin2014}. However, the symmetry between the openings may not favor the respective method, unless the imaging is done by two different platforms or the architectural style be a well defined facade layout. 

In terms of architectural style classification, recent works such as \cite{mathias2011}, \cite{henn2012}, and \cite{weissenberg2014}, has addressed the problem to the first stage in a 3D reconstruction methodology: first, to identify what to deal with - Is it a residential area? Is it industrial? The success of any method depends solely on the distribution, testing and execution of micro tasks. In this case, the classification of the facade geometry is a micro task of 3D reconstruction, which even though is not approached in this study, it is important to emphasize the need of this stage as a pre-step in 3D reconstruction issue. 

The methodology presented here does not include facade layouts categorization. Instead, it is intended to segment facade features by using Machine Learning (ML) techniques, which has proven they are robust under complex scenarios, such as undefined architectural styles or areas occluded by obstacles. The method considers not only multi-scale analysis, but it is also sensitive towards context, when objects obstructing doors or walls, for example, are easily ignored by the neural model when the obstruction is small. 

\subsection{Architectural styles}\label{architectural-style}
In fact, the number of possibilities to get around these scenarios is infinite. The range of alternatives, however, can grow even more according to the regions in which it is mapped in the city. It is clear, therefore, that industrial sectors have more differentiated structures than residential or commercial sectors. These, however, are more critical because they are sectors of higher traffic, in general with dynamic structures, greater density that, in the end, make difficult for both to  access (at acquisition) and to autonomous extract the features.

Looking at the architectural styles shown in Figure \ref{facade-layouts}, one can see that the layout of facade features for these styles differ drastically and in some cases is hardly detectable by an automatic method.
\begin{figure}[H]
    \centering     
    \caption{Examples of architectural styles according to different countries and regions in the city. (a) Hausmaniann. (b) Neoclassic. (c) Religious. (d) Modern. (e) Free-form. (f) Vertical-gardens. (g) \emph{Aglomerados subnormais - favelas}.}    
    \subfigure[]{\label{facade-layoutsa}\includegraphics[width=0.186\textwidth]{\dropbox/phd/pics/archs/hausmaniann.jpg}}
    \subfigure[]{\label{facade-layoutsb}\includegraphics[width=0.34\textwidth]{\dropbox/phd/pics/archs/neoclassic.png}}    
    \subfigure[]{\label{facade-layoutsc}\includegraphics[width=0.195\textwidth]{\dropbox/phd/pics/archs/religious.png}}
    \subfigure[]{\label{facade-layoutsd}\includegraphics[width=0.23\textwidth]{\dropbox/phd/pics/archs/modern.png}}
    \subfigure[]{\label{facade-layoutse}\includegraphics[width=0.35\textwidth]{\dropbox/phd/pics/archs/free-form.png}}
    \subfigure[]{\label{facade-layoutsf}\includegraphics[width=0.33\textwidth]{\dropbox/phd/pics/archs/vertical-garden.png}}
    \subfigure[]{\label{facade-layoutsg}\includegraphics[width=0.3\textwidth]{\dropbox/phd/pics/archs/favela.png}}
    \legenda{} 
    \label{facade-layouts}  
    \FONTE{Images from multiple sources on internet. Unknown authors.}
\end{figure}
 
The Haussmanian style, in Figure \ref{facade-layoutsa}, was the result of a French renovation during the 1870s and represents the predominant style in France, with straight facades, with around 4 to 5 floors, windows often together with balconies, ground floor with the presence of shops, with clear walls and well defined textures. This style, as well as Classicism and Historicism (Figure \ref{facade-layoutsb} and \ref{facade-layoutsc}) also present in many European countries (e.g. Austria, Germany and Spain), is also characterized by the symmetry among the facade features, such that by the layout of the windows and roof, would be possible to estimate the number of floors or amount of internal luminosity of each sector. These styles make up part of the datasets used in this study and in the literature regarding facade feature detection and 3D reconstruction.

Figure \ref{facade-layoutsd} to \ref{facade-layoutsg}, on the other hand, presents examples of complex architectural geometries that, in terms of autonomous methods of detection and reconstruction, require approaches different from those treated in this study. That is, it is evident that methodologies dealing with the extraction and reconstruction of facade features in Hausmaniann architectures would certainly fail on modern architectures. However, these dialogues are difficult to find in the literature that, in certain moments discard this question. Modern architectures have glass in abundance, not only in windows, but also on every surface of their walls. Reflexive surfaces, for example, have features that render the SfM/MVS method useless.

Modern architectures, as in Figure \ref{facade-layoutse}, demarcate another stage of reconstruction. These structures, because they are atypical, do not require automatic methods of reconstruction. They are usually mapped or generated artistically. Not only vertical gardens, in Figure \ref{facade-layoutsf}, but also terraced gardens, are becoming common throughout the years. A sustainable practice that tries to recover the space occupied by constructions, but which represent a difficulty in 3D reconstruction. Its texture is confused with the undergrowth or arboreal vegetation which, for methods that use this metric, can be easily confused.

Finally, although this building does not have a defined architecture, it represents one of the most important structures that, once reconstructed, would greatly benefit 3D mapping in Brazil: the subnormal clusters (in Portuguese, \emph{aglomerados subnormais}), popularly called favelas. These buildings are difficult to access, usually accessible only by walking. By aerial campaigns, they are difficult to map by their spectral characteristics, they do not present any symmetry or pattern. However, they have cultural centers, a large part of the population lives in these areas, it is impossible to measure issues such as sanitation or disasters, attacks on public safety, among others.

It is necessary to attack smaller causes, and science, as in any area, will merge each of these branches according to their context. Although this study has approached the analysis of different scenarios, including complexes, it is stated that the 3D reconstruction issue must be treated according to the city or region in which intends to study. Urban environments are extremely dense and complex environments. The conception of a generic model that draws attention to all these disparities is impractical. However, constant observation of these structures narrows down the range of options and encourages future studies. Bringing a natural evolution of such routines.

\subsection{Standardization - Level of Details (LoDs)}\label{padronização}  
The Open Geospatial Consortium (OGC)\footnote{Association of companies, universities and government agencies for the development of publicly available geospatial interface standards to support interoperable solutions, making geospatial services accessible and useful \cite{ogcstandard}.} with the purpose of standardizing the file formats and quantifying different levels of details of 3D models, established the CityGML language, which provides support for the declaration of physical characteristics and relations between urban elements. 

The language consists of a common semantic model for 3D representations of urban environments. Developed as open-source in XML (Extensible Markup Language) format, the standard adopts the Geography Markup Language 3, which represents the international standard for spatial data exchange and coding, idealized by OGC and ISO TC211 \cite{kresse2004}. The standard establishes LoDs consisting of degrees that each 3D object is represented. Objects in the same scene may have higher or lower degrees than others.

LoDs are classified into five levels: first, LoD0, corresponds to the basic level of detail, comprising the planimetric information. The second level, LoD1, corresponds to the 3D representation, simple extrusions, where the buildings are represented not only by its spatial location, but also by its height. At the LoD2 level, simple structural features such as external columns or garages are aggregated. The LoD3, presents more refined external structural details. At this level, texture components and openings are added to their facades, such as doors, windows and balconies. Finally, the LoD4 represents the highest level, with information on the internal structure, usually, acquired by indoor instruments \cite{kolbe2005} (Figure \ref{lods}).
\begin{figure}[H]
    \centering     
    \caption{Levels of detail (LoDs) of the standard \emph{CityGML} for 3D buildings models.}
    \vspace{6mm}
    \includegraphics[width=1\textwidth]{\figspath/lods/lods.pdf}    
    \legenda{}
    \label{lods}   
    \FONTE{Adapted from \citeonline{kolbe2005}.}
\end{figure}

Even though there are other standards such as COLLADA (COLLAborative Design Activity) and KML (Keyhole Markup Language), the OCG CityGML standard is particularly the most adopted. CityGML mainly describes the geometry, attributes and semantics of different kinds of 3D urban elements. These can be supplemented with textures or colours in order to give a better impression of their appearance. In addition, semantic information can also be provided, where a description of the geometry, attributes and relationship between different objects can be specified, for example, a building can be decomposed into three parts, or has a safe or a balcony. 

\subsection{Advances in 3D urban reconstruction}\label{monitoring-cities}
The use of high resolution images has been the most effective method for systematic monitoring in all contexts, be it forest, ocean or city. Understanding patterns of changes over time is, however, a task that requires enormous effort when executed manually. In addition, human touch is susceptible to failure and could require prior experience in target perception. As matter of fact, so far few works have been carried out in the field of architectural style identification or facade feature extraction and reconstruction \cite{martinovic2015}. Research has reached a certain level of maturity today, with a variety of technologies for acquisition, high graphics processing, storage and dissemination of information that is available for the automatic operators to make use of. 

The development of intelligent operators for image labeling can be categorized into model-free, model-based, and procedural-models. The first, classical segmentation methods such as Normalized Cuts (NC) \cite{shi2000}, Markov Random Fields (MRF) \cite{kolmogorov2004}, Mean Shift (MS) \cite{comaniciu2002}, Superpixel \cite{achanta2012}, and Active Contours (ACon) \cite{kass}, they do not consider the shape of objects or their spectral characteristics, which, in practice, means they always fail in regions where the elements of a same facade do not share the same spectral attributes. Model-based or parametric-model operators use a prior knowledge base, which together with segmentation procedures, provide more consistent results on a given region. However, this knowledge is finite and opens up new possibilities for failures when applied in regions with different characteristics. The third and last, procedural-models like Grammar Shape \cite{stiny1971}, comprise the group of rule-based methods, in which algorithms are applied to the production of geometric forms \cite{teboul2011}.

Image segmenters that carry some intelligence usually carry issues with them as well. First, it is necessary to configure and train the model, then, make inferences in what each pixel or region corresponds to. \cite{teboul2011} proposed a Grammar-based procedure to segment building facades, where a finite number of architectural styles are considered. The proposed method is able to classify a wide variety of facade layouts and its features using a Tree-based classifier, which improves the detection with only a small percentage of false negatives. 

The Grammar-based approaches, however, are normally formulated by rules which follow common characteristics, such as the sequentiality, which is normally present on \textquotedblleft Manhattan-world\textquotedblright$~$or European styles \cite{wenzel2008}, where the shapes found seem to have lower geometric accuracy since the 3D model is generated, not reconstructed. Still, the outcomes of Grammar-based approaches provide simplicity and perfectly resume the real scene. Other similar works, such as \cite{becker2009}, \cite{nan2010}, \cite{wan2012}, and \cite{boulch2013}, have proposed equivalents solutions to the problem, but still lies on the same deficiencies mentioned above.

In addition to procedural modeling, urban 3D reconstruction incorporates a new class of research, one based on physical (structural) measurements, which comprise measurements by laser scanners and MVS workflows (mainly). As far as it is known, in this category lie the works that present the most consistent methodologies and results, which take into account the geometric accuracy, where the classification of objects can also be explored by their shapes or volume, in addition to their spectral information. 

\citeonline{jampani2015} and \citeonline{gadde2017}, respectively proposed a 2D and 3D segmentation based on Auto-Context (AC) classifier \cite{tu2010}. The facade features are explored by its spectral attributes and then, iteratively refined until results are acceptable. The AC classifier applied in a urban environment is a good choice since it considers the vicinity contribution, which is essential in this particular scenario. The downside, however, is that in this case the AC only succeeds when the feature detection (based on spectral attributes) is good enough, otherwise, it could demand many AC stages to get an acceptable output.

Unlike the approaches mentioned above, there are also lines of research that address the problem of 3D reconstruction over the mesh itself, for this reason, other structural data can be explored, such as LiDAR. \citeonline{lafarge2011, verdie2015, oesau2016} present different contributions, however, with strictly related focuses. It should be noted, therefore, that the approaches in this line of research demand complex geometric operations, for instance, regularities using parallelism, coplanarity or orthogonality. These operations usually have refinement purposes, and also give the 3D modeling an alternative to acquire more consistent and simplified models.

Automatic 3D reconstruction from images using SfM/MVS workflows is challenging due to the non-uniformity of the point cloud, and it might contains higher levels of noise when compared to laser scanners. In addition to that, missing data is an unavoidable problem during data acquisition due to occlusions, lighting conditions, and the trajectory planning \cite{li2016}. The following mentioned papers explore what it is understood as the best methodologies in 3D reconstruction, according to our established goals: exploring the texture first, and then acquiring the semantic 3D model \cite{brostow2008}. \citeonline{martinovic2015} proposed an end-to-end facade modeling technique by combining image classification and semi-dense point clouds. 

As seen in \citeonline{hayko2014} and \citeonline{bodis2017}, the respective approaches have motivated us in the sense that facade 2D information can be explored more thoroughly in order to improve its volumetry reconstruction. \citeonline{martinovic2015}, for instance, uses the extracted facade features to analyze the alignment among them, where a simple discontinuity shows the boundaries between different facades. Thereby, it could be used to pre-classify subareas, such as residential, commercial, industrial, and others. \citeonline{sengupta2013}, \citeonline{hayko2014} and \citeonline{bodis2017}, similarly, explored different spectral attributes in order to discriminate the facade features as well as possible. Moreover, the 3D modeling is later supported by these outcomes by performing a complex regularization and refinements over the mesh faces.

\section{3D mapping around the World}\label{map-mundo}
Investigating the exact number of cities that actually use 3D urban models as strategic tool in their daily lives can be a difficult task. However, \cite{biljecki2015a} presented a consistent review of entities (industry, government agencies, schools and others) that make or made use of 3D maps beyond the visual purpose. Hence, only applications supported by the respective technology are, in fact, listed. Examples of such applications are the visibility analysis for security cameras installation \cite{ming2002, yaagoubi2015}, urban planning \cite{sabri2015, leszek2015}, air quality analysis \cite{amorim2012, janssen2013}, evacuation plans in emergency situations \cite{kwan2005}, urban inventories with cadastral updating \cite{qin2014}, among others.

Although the number of cities adopting this tool is uncertain, some of these are known for their technological advances and social development, an important indicator in the implementation of innovative projects. Countries such as the United States, Canada, France, Germany, Switzerland, England, China and Japan are among the leading suppliers of Earth Observation equipment, for example, Leica Geosystems\texttrademark~(Switzerland) laser systems, FARO\texttrademark~(USA), Zoller-Fröhlich\texttrademark~(Germany), RIEGL Laser Measurement Systems\texttrademark~(Austria), Trimble Inc.\texttrademark~(USA), TOPCON\texttrademark~(Japan), and countless optical sensors used in ground and airborne surveys. It is natural, therefore, that these great providers also become reference in conducting research in the sector.

In Germany, the so-called \textquotedblleft city-models\textquotedblright~were built with the basic purpose of assisting and visualizing simple scenarios or critical situations. At that time, these models did not have sufficient quality for certain analyzes or permanent updating, making use of the old 2D registers for queries. In the end, the 3D models never became part of the register. The concept of urban 3D reconstruction has become, due to demand, a scientific trend in Cartographic, Photogrammetry and Remote Sensing almost everywhere in the world, especially in the aforementioned countries. 

Naturally, new questions arose. How to merge information already available in 2D databases with the ones in 3D? In certain circumstances, what is the limit on the use of 3D information? When the 2D is already enough and when not? \citeonline{biljecki2015a} argue that all applications that require 2D information can be solved with 3D, but that does not make it a unique feature, but an optional one. For example, \citeonline{kluijver2003} carried out a study of the propagation of noise in urban environments from 2D data. Years later, in \citeonline{stoter2008}, the study was complemented with 3D information, showing considerable improvement in the estimation.


\section{3D mapping in Brazil}\label{map-brazil}
In Brazil, the Geographic Service Directorate (in Portuguese, \emph{Diretoria de Serviço Geográfico} (DSG))\footnote{Available at http://www.dsg.eb.mil.br. Accessed \today.} is the unit of the Brazilian Army responsible for establishing Brazilian cartographic standards for the 1: 250,000 and larger scales, which implies standardizing the representation of urban space for basic reference mapping. Recently, the National Commission of Cartography (CONCAR) has put forward the new version of the Technical Specification for Structuring of Vector Geospatial Data (ET-EDGV) \cite{concar2016}, which standardizes reference geoinformation structures from the 1:1000 scale. The data on this scale serve as a basis for the planning and management of the urban geographic Brazilian space. In Brazil, the demand for 3D urban mapping is still low and faces challenges that go beyond its standardization.

As documented in this section, the state-of-art in automatic 3D urban reconstruction covers areas with moderate modeling, whose architectural styles are very specific, streets with large spacing, or symmetry between facade elements, which makes the creation of automatic methods a bit more feasible \cite{vangool2013}. In countries where there is a high density of buildings, such as Brazil, India, or China, this factor is aggravated by the urban geometry. Many of the Brazilian cities do not have a specific style. In suburbs, for example, this factor can prove even more aggravating, where settlement areas or subnormal settlements (in Portuguese, \emph{favelas}) are all built under these circumstances, with high density and sometimes, erected irregularly or over risky areas, which makes them even more prominent in mapping. 

Initiatives such as the TáNoMapa, by Grupo Cultural AfroReggae\footnote{Available at https://www.afroreggae.org/ta-no-mapa/. Accessed \today.}, together with the North American company Google\texttrademark, consist of mapping hard-to-reach areas such as streets with narrow paths, cliff, among others, by the own local residents. Such areas, in addition to being geometrically complex, require not only cooperation from the government but also from the community who lives there, which by social or security reasons, may require some consent.

Even though it faces many obstacles, Brazilian urban mapping is moving towards more sophisticated levels. In 2016, the National Civil Aviation Agency (in Portuguese, \emph{Agência Nacional de Aviação Civil} (ANAC)) regulated the use of UAVs for recreational, corporate, commercial or experimental use (Brazilian Civil Aviation Regulation, in Portuguese, \emph{Regulamentos Brasileiros da Aviação Civil} (RBAC), portaria E~n\textordmasculine~94 \cite{anac-portaria}). The regulation, widely discussed with society, associations, companies and public agencies, establishes limits that still follow the definitions established by other civil aviation entities such as the Federal Aviation Administration (FAA), the Civil Aviation Safety Authority (CASA) and the European Aviation Safety Agency (EASA), regulators from the United States, Australia and the European Union, respectively \cite{anac}. Thus, close-range acquisitions through the use of UAVs became feasible and have fostered research in these fields.

\section{Geometry extraction}    
\subsection{Structure-from-Motion (SfM)}\label{sfm}
The drawbacks of 3D reconstruction by Photogrammetry are not so many when take in account its advantages \cite{remondino2005}. In the past decades, close and long-range Photogrammetry have become a widely used tool in 3D city modeling \cite{remondino2006, fraser2009}. Its low costs and availability in remote areas increased its demand, especially in the analysis of such environments. The SfM technique is based on the same physical principles of Stereoscopy, in which a structure can be reconstructed from a serie of images taken at small positional variations and overlapping (Figure \ref{sfm-camera}). In conventional Photogrammetry, however, both the position and orientation of the sensor must be known. By the SfM technique, the positional parameters can be estimated. In contrast, point clouds generated by SfM does not have a coordinate system. All points are generated in image space, which must later be aligned to the coordinate system of the object \cite{westoby2012}. 
\begin{figure}[H]
    \centering 
    \caption{Structure-from-Motion and camera projection. Instead of a single stereo pair, the SfM technique requires multiple, overlapping images as input to feature extraction and 3D reconstruction algorithms.}
    \vspace{6mm}
    \includegraphics[width=1\textwidth]{\figspath/sfm/schema/camera-sfm.png}         
    \vspace{-6mm}
	\legenda{}	
    \label{sfm-camera}       
    \FONTE{Adapted from \citeonline{westoby2012}.}   
\end{figure}

The workflow to obtain the point cloud using SfM begins with the detection of corners or keypoints in all input images. These points are then used as descriptors in a correspondence analysis among other images, also known as Image-Matching (IM). Pairs are formed according to the level of correlation between each descriptor (i.e., which feature points in several different images depict the same 3D point on the original object). Once the matching has been concluded, the camera orientation and position estimation is started. With the estimated parameters, the matching points now have depth, which together provides detailed structural information of the entire imaged environment. 

The camera position and orientation are estimated iteratively in a process called Bundle Adjustment (BA) \cite{triggs1999}, which in turn, requires a set of overlapping images and sharing the same set of features. The procedure consists of refining the 3D coordinates based on the number of overlapping images and the reflection of the light rays converging on each camera projection center. The level of overlap and alignment, therefore, determine the quality of estimation (geometric accuracy) of the point cloud in any SfM methodology \cite{snavely2008, hofer2016}.

Different approaches for SfM pipelines has change the way how to 3D reconstruct cities. As summarized in the Table \ref{tabela-dado-estrutural}, specialists on image acquisitions are no more required since the pictures available on internet could provide a sufficient dataset for 3D modeling. For instance, the Bundler \cite{snavely2008} takes an unorganized image collection as input, then it is able to reorganized them in a way that each pair correspond to an overlapped image. Although it only solves the problem mostly at touristic sites, the idea of using images from ordinary users has stimulated areas such as Cartography, Photogrammetry and Remote Sensing since then. Another example, the VarCity project\footnote{Details about the project are available at https://varcity.ethz.ch/. Accessed \today.} \citeonline{hayko2014}, which uses information from a variety of internet sources and integrates them to reconstruct every scenario in 3D.

Keypoints are simply strategic points denoting the image identity, areas with optimal characteristics for correlating two images, free of spectral and spatial variations, for example, sharp edges and corners. Fundamental in the areas of Computer Vision, Photogrammetry and Remote Sensing, the concept of corner detection is old and is based on the analysis of images in differential space. The issue, in this case, is how to detect corners in one image and at the same time correlate it with another one, which in turn can present variations in scale, texture or orientation? First, \citeonline{harris1988} proposed to find keypoints using measures based on eigenvalues of smoothed gradients, which allowed the detection of corners independently of rotation, translation and brightness, but not in depth (scale). Then, in 2004, \citeonline{lowe2004} proposed the Scale Invariant Feature Transform (SIFT) method, solving the scale problem and also adding the IM resolution.

\citeonline{yuan2017} categorizes the matching algorithms in two segments, the ones based only in radiometric information and those based either on radiometric and geometric information. Respectively examples of approaches are the Normalized Cross Correlation (NCC) \cite{gonzalez1992}, Scale Invariant Feature Transform (SIFT) \cite{lowe2004}, and the Distinctive Order Based Self-Similarity (DOBSS) \cite{sedaghat2015}, which stands in the high quality of the images. The second group of algorithms include the Semi-global Matching (SGM) \cite{heiko2008}, Path-based Multi-View Stereos (PMVS) \cite{furukawa2007pvms}, and Multi-photo Geometrically Constrained Matching (MGCM) \cite{zhang2006}. These last algorithms take advantage not only from fine radiometric information, but also from the predetermined geometric information, functions as a supplementary matching constraint.

Among these, SIFT has become the most popular. SIFT is independent of scales, variations of brightness, contrast and orientation. After identified, the matching points are determined in pairs of images according to the correlation of local descriptors in each point. A common descriptor is the use of the HOG, which considers the magnitude and orientation of its neighbors as an identifier in the matching process. In the case of low correlation, these are discarded by the algorithm. The size and quantity of images are also part of the problem in the identification of keypoints, depending, demanding days of processing. More sophisticated procedures have improved SIFT by the use of GPUs (Graphic Processing Unit)\footnote{Electronic circuit dedicated to high-performance graphics processing.}, for example, SiftGPU \cite {wu2007}.

Since all geometric constraints are applied, the location of the 3D points related to the cameras as well as their positioning can then be estimated \cite{berjon2016, snavely2008}. The position and orientation parameters of the camera are estimated iteratively in BA \cite{triggs1999} which, in turn, requires a set of overlapping images and sharing the same set of features. The procedure consists of refining the 3D coordinates based on the number of overlapping images and the reflection of the light beams converging on each center of the camera. The degree of overlap and alignment, therefore, benefits the estimation of the parameters and determines the quality (geometric accuracy) of the point cloud in the whole SfM procedure \cite{snavely2008, hofer2016, haala2014}.

Some of the SfM stages have been improved over the years. Examples of these transformations are the recent use of BA for the iterative adjustment of camera positioning, the use of SiftGPU \cite{wu2007} for computational optimization for image-matching and Multicore Bundle Adjustment (MBA) \cite{wu2011}, the technique Clustering Views for Multi-View Stereo (CMVS) \cite{furukawa2010cmvs}, which the entries are decomposed into smaller processing groups in order to enable parallel processes, PMVS-2 \cite{furukawa2010pmvs2}, a improved version of PMVS, responsible for the automatic reconstruction of only rigid objects, pedestrians, moving cars and others. In Figure \ref{sfm-workflow}, SfM is summarized as described above. The post-processing phase is composed of the dense point cloud refinement, where points can be simplified, filtered (noise removal), or used as input for surface reconstruction (e.g. Poisson \cite{hoppe2008}) and texturing.
\begin{figure}[H]
    \centering     
    \caption{Most recent Structure-from-Motion pipelines. From photographs to sparse point-clouds.}    
    \vspace{6mm}
    \includegraphics[width=0.80\textwidth]{\figspath/sfm/flow/sfm-flow-en.pdf}
    \vspace{2mm}
    \legenda{}
    \label{sfm-workflow}       
    \FONTE{Adapted from \citeonline{yuan2017, westoby2012}.}
\end{figure}

\section{Deep Learning (DL)}\label{deep-learning}
In terms of image labeling, years of advances have brought what is now considered a gold-standard in segmentation and classification: the use of DL. The technology is the new way to solve old problems in Remote Sensing \cite{audebert2017}. It is one of the branches of ML that allows computational models with multiple processing layers to learn representations in multiple levels of abstraction. The term \textquotedblleft deep\textquotedblright, refers to the amount of processing layers that is usually used during training. 

\citeonline{mitchell1997} succinctly defines the learning from a ML as: \textquotedblleft A computer program learns from a given experience $E$ relative to application classes $T$ with a performance $P$, if the performance on task $T$ is better than in $E$ \textquotedblright. Widely categorized as unsupervised or supervised, machine learning is defined by the type of experience that can be applied during the learning process. Shortly, unsupervised learning does not require samples during the training process (learning occurs with the dataset itself), while the supervised assume its behavior according to a reference set. \citeonline{lecun2015} believe that unsupervised learning will become far more important in the longer term, since machine learning tends to look more like human learning, which in turn has its learning largely unsupervised: \textquotedblleft \textit{they discover the structure of the world by observing it, not by being told the name of every object}\textquotedblright.

These models have made remarkable advances in the state-of-art of pattern recognition, speech recognition, detection of objects, faces, and others. Shortly, DLs are trained to recognize structures in a massive amount of data using, for example, supervised learning with the concept of backpropagation\footnote{Backpropagation is a method commonly used in ML to calculate the error contribution of each neuron after each training iteration.}, where the model is indicated the proportion of what must be changed in each of its layers until \textquotedblleft learning\textquotedblright~occurs (error decay) \cite{lecun2015}.

Unsupervised learning had a catalystic effect in reviving interest in DL, since then, it has been overshadowed by the successes of supervised learning. \citeonline{lecun2015}, on the other hand, believes that unsupervised learning will become far more important in the longer term, as the technological tendency is to get closer and closer to human behavior. Human and animal learning is largely unsupervised: they discover the structure of the world by observing it, not by being told the name of every object.

\subsection{Convolutional Neural Network (CNN)}\label{cnn-sect}
In 1943, the first Artificial Neural Network (ANN) appeared \cite{pitts1943}. With only a few connections, the authors were able to demonstrate how a computer could simulate the human learning process. In 1968, \cite{hubel1968} proposed an explanation for the way in which mammals visually perceive the world using a layered architecture of neurons in their brain. Then, in 1989, a neural model started to get attention not only because of its results, but also for its similarities with the biological visual system, with processing and sensation modules. 

\citeonline{lecun1989} presented a sophisticated neural model to recognize handwritten characters, named first LeNet, then later, Convolutional Neural Network (CNN). Precisely, the reason for this name is due the successive mathematical operations of image convolutions. Since then, many engineers have been inspired by the development of similar algorithms for pattern recognition in Computer Vision. Different models have emerged and contributed to the evolved state of neural networks in the present days. In Figure \ref{cnn-arch}, the LeNet neural model, developed by Yann Lecun in 1989 \cite{lecun1989}.
\begin{figure}[H]
    \centering     
    \caption{LeNet: The first version of a CNN, projected by Yann Lecun in 1989.}
    \vspace{6mm}
    \includegraphics[width=1\textwidth]{\figspath/cnn/lenet/leNet_1.pdf}
    \includegraphics[width=1\textwidth]{\figspath/cnn/lenet/leNet_2.pdf}
    \legenda{}    
    \label{cnn-arch}   
    \FONTE{Adapted from \citeonline{lecun1989}.}
\end{figure}

The CNN are structured in stages, the first ones, are composed of two types of layers: convolutional and pooling (in the figure, in blue and red). Units in a convolutional layer are organized into feature maps or filters, where each unit is connected to a window (also called patch $C_x$ - details in the bottom figure) in the feature map of the previous layer. The connection between the window and the feature map is given by weights ($w$) and biases ($b$). The weighted sum of the convolution operations is followed by a nonlinear activation function, called Rectified Linear Unit (ReLU). For many years, activations in neural networks were composed of smoother functions, such as the $tanh(x)$ or $1/(1+e^{-x})$ sigmoids, but recent study has shown ReLU to be faster on learning in multilayered architectures \cite{lecun2015}. The fully-connected layer correspond to a traditional Multi-Layer Perceptron (MLP), with hidden layer and logistic regression. Then, the input to the MLP layer is the set of all features maps at the previous one.

CNN have made advances in image, video, speech and audio analysis, while Recurrent Networks (RR) have supported the path to sequential data such as text and speech \cite{lecun2015}. It was designed to recognize visual patterns directly from pixel images with minimal preprocessing. Due to convolution over images and its reduced versions (downsampling), they can recognize patterns with extreme variability, scales, and with robustness to distortions and simple geometric transformations such as handwritten characters. 

Because of its robustness in image classification and segmentation over complex environments, CNN has been a trend in many Computer Vision applications. For example, the approach is considered gold-standard in applications such as face recognition \cite{lawrence1997}, speech recognition \cite{abdel2013}, natural language \cite{kalchbrenner2014, ciresan2011}, and others. In image analysis, not the first, but the reference on the use of CNNs for images can be named to the model AlexNet \cite{krizhevsky2012}, when the technique began to be exhaustively tested and became a practical and fast way in object classification. 

With the success on ordinary image classification, neural models came to be used also in numerous applications involving Remote Sensing \cite{zhu2017}. Examples of that are the analysis of orbital images \cite{castelluccio2015, marmanis2016}, radar \cite{chen2014sar, chen2016, sun2012}, hyperspectral \cite{chen2014, romero2014} and in urban 3D reconstruction \cite{hane2013, blaha2016, blaha2016towards}. Although not focused specifically on the analysis of facades, excellent results have been reported in the classification of urban elements through the use of DL. 

An example of this evolution can be observed in the annual PASCAL VOC \cite{pascalvoc} challenge, brings together experts to solve classical tasks in Computer Vision and related areas. The applications range from recognition \cite{krizhevsky2012} to environment understanding, where the analysis is focused on the relationship between the objects themselves. Therefore, certain constraints could be imposed to the relation, for example, between a pedestrian and street or vehicles in applications involving self-driving \cite{segnet, teichmann2016}, such that a distance and speed constraints could be imposed between these detected objects. \citeonline{lettry2017} used CNN to detect repeating features in rectified facade images, wherein the repeated patterns are verified on a projected grid, then, it is used as a device to detect those regular characteristics and reconstruct the scene. 

\subsubsection{Autoencoder}
How humans perceive the environment around them is still a topic of recurrent research and, so far, not fully understood. The human visual system consists basically of two processes: radiation capturing and visual perception. The first, numerous chemical and optical processes take place in the eyes of the observer. All visible radiation captured is transformed into synaptic signals by photo-receptor cells, called cones and rods. From this point, these signals representing this lapse moment are then directed to specific regions of the brain, which through chemical reactions and many others to be understood, allow to connect these signals to the perceptual neural senses, which then allows to see and perceive the environment and its environment. Completing the basic mechanism of the human visual system.

CNNs can assume different processing mechanisms, different numbers of layers and how they are connected. The architecture called Autoencoders or encoder-decoder \cite{segnet, teichmann2016}, presents similarity not only in architecture but also in the behavior of human visual system. As described in the previous paragraph, two basic mechanisms can be noticed in the biological system: one for processing the radiation in synaptic signals by photo-receptor neurons; and another, for environmental perception, mainly performed by the visual cortex. 

Similarly, the encoder and decoder mechanism respectively simulate this two process. First, the input image is subjected to numerous convolution and pooling operations. For each pooling, the downsampling process occurs, where the image resolution is reduced and passed on as input to the subsequent convolutional layer. Consequently, the filter bank learn different patterns in different scale, orientation and location (functions similar to those of the biological lens and retina). Signals in small dimensions are then transmitted to the decoder, which from three convolutional layers performs the upsampling, the transformation of signals from smaller resolutions to larger resolutions. The result of this transformation is therefore a map of \textquotedblleft perceptions\textquotedblright, the segmented image (Figure \ref{encoder-decoder-arch}). 
\begin{figure}[H]
    \centering     
    \caption{Autoencoder architecture to segment images.}
    \vspace{6mm}
    \includegraphics[width=1\textwidth]{\figspath/cnn/flow/cnn-flow.pdf}
    \legenda{}
    \vspace{2mm}
    \FONTE{Adapted from \citeonline{segnet}.}      
    \label{encoder-decoder-arch}
\end{figure} 

Another feature of these systems and also idealized observing human behavior is the way humans perceive space and objects in their environment. In a lapse of time, innumerable processes are triggered instantly in the brain. In one of these, it is possible to note the human capacity to identify distinct features of all the rest. It is like the human visual system \textquotedblleft would not worry\textquotedblright~at capturing all the information on that environment, but rather, only those that might be interesting. Permanently, the human eyes scan the environment in very fast movements, process called Saccade \cite{ko2016, pirkl2016}. These very fast movements, allow the eyes to capture fractions of images step-by-step. In the Figure \ref{feature-imagem-trajeto-retina}, for instance, the features highlighted by the CNN and the path of the iris when exposing a random image is accurately close.
\begin{figure}[H]
 \centering     
 \subfigure[]{\label{feature-a}\includegraphics[width=0.48\textwidth]{\figspath/cnn/feature/feature-a.png}
 \includegraphics[width=0.48\textwidth]{\figspath/cnn/feature/feature-b.png}}\\
 \subfigure[]{\label{feature-b}\includegraphics[width=0.475\textwidth]{\figspath/eyetracking/eyetracking.png}
 \includegraphics[width=0.50\textwidth]{\figspath/eyetracking/eye-tracking-2.png}}
 \caption{Analogy to the computational neural system and biological. (a) Output of a Recurrent Neural Network (RNR), a special type of \emph{deep} network. The network is trained to translate high levels of representations into texts. In the figure, the network's ability to focus its attention on specific sections of the image; (b) Eye-tracking device and the capture (gray points) of eyeball movements - Saccade effect.}
 \label{feature-imagem-trajeto-retina}
 \FONTE{Adapted from \citeonline{lecun2015, pirkl2016, dalmaijer2014}.}             
\end{figure} 

The semantic segmentation of images by the use of neural networks has gained adherents in a wide range of applications, ranging from the recognition of objects of different natures \cite{krizhevsky2012}, as well as the understanding of the environment, in which the analysis is also applied on the relation between the objects themselves. Thus, certain constraints could be easily applied, for example, a pedestrian with the object street or automobiles in an application involving autonomous driving \cite{segnet, teichmann2016, lettry2017}. As emphasized by \citeonline{zhu2017}, Remote Sensing data represent a new challenge for DL. Their imaging nature can often be differentiated. For example, optical, laser and radar data provide different representations, which would require analysis that has not yet been so explored with the use of DL. Remote Sensing data is georeferenced, images from internet are usually free of location metadata. In this case, DL could correlate specific facades from internet to a location in a SR data, supporting Security agencies.

The DL as an automatic extractor of urban features is a scientific question of great interest to the community and also covers a limited number of works. The efforts so far, shows that there is progress in identifying facade features in specific architectural layouts, with well-defined, symmetrical, and accessible modeling facades. The use of benchmark datasets, such as the ones used in this study, is common and provide a wide overview about the extraction algorithms available today. 

In \citeonline{liu2017deepfacade}, for example, DL is used for the identification of facade features on two online datasets: eTRIMS and ECP datasets, presenting similar results to those shown in this study. The VarCity project \cite{hayko2014}\footnote{Available at https://varcity.ethz.ch/index.html. Accessed \today.}, provides accurate perspective on 3D cities and image-based reconstruction. The research involves not only studies in \textquotedblleft how\textquotedblright~to reconstruct, but how these semantic models could automatically assist to the daily life events (e.g. traffic, pedestrians, vehicles, green areas, among others).
